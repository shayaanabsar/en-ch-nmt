{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os import path\n",
    "from math import sqrt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\tdef __init__(self):\n",
    "\t\tself.vocabulary = set()\n",
    "\t\tself.stoi = {'<N>':0}\n",
    "\t\tself.itos = {0:'<N>'}\n",
    "\n",
    "\tdef add(self, v):\n",
    "\t\tif type(v) == str:\n",
    "\t\t\tself.vocabulary.add(v)\n",
    "\t\telif type(v) == list:\n",
    "\t\t\tself.vocabulary = self.vocabulary.union(set(v)) \n",
    "\n",
    "\tdef create_mappings(self):\n",
    "\t\tself.stoi |= {v:i+len(self.stoi) for i, v in enumerate(self.vocabulary)}\n",
    "\t\tself.itos |= {i+len(self.itos):v for i, v in enumerate(self.vocabulary)}\n",
    "\n",
    "\tdef encode(self, s): \n",
    "\t\treturn [self.stoi[c] for c in s]\n",
    "\t\n",
    "\tdef decode(self, i): \n",
    "\t\treturn [self.itos[n] for n in i]\n",
    "\t\n",
    "\n",
    "class PreProcessor:\n",
    "\tdef __init__(self):\n",
    "\t\tself.english_vocabulary = Vocabulary()\n",
    "\t\tself.cherokee_vocabulary = Vocabulary()\n",
    "\t\tself.cherokee = []\n",
    "\t\tself.english = []\n",
    "\t\tself.max_length = 0\n",
    "\t\tself.count = 0\n",
    "\n",
    "\tdef load_text(self, file_name):\n",
    "\t\tdata, language = [], file_name.split('.')[0]\n",
    "\n",
    "\t\twith open(path.join('chr_en_data', file_name)) as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tsentence = ['<S>'] + word_tokenize(line) + ['<E>']\n",
    "\n",
    "\t\t\t\tif language == 'en':\n",
    "\t\t\t\t\tself.english_vocabulary.add(sentence)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.cherokee_vocabulary.add(sentence)\n",
    "\n",
    "\t\t\t\tself.max_length = max(self.max_length, len(sentence))\n",
    "\t\t\t\tdata.append(sentence)\n",
    "\t\t\t\tself.count += 1 \n",
    "\t\treturn data\n",
    "\t\n",
    "\tdef get_data(self):\n",
    "\t\tcherokee = self.load_text('chr.txt')\n",
    "\t\tenglish  = self.load_text('en.txt' )\n",
    "\t\tassert len(cherokee) == len(english)\n",
    "\t\tself.cherokee += cherokee\n",
    "\t\tself.english  += english\n",
    "\n",
    "\t\treturn cherokee, english\n",
    "\n",
    "\t\n",
    "\tdef create_tensors(self):\n",
    "\t\tself.english_vocabulary.create_mappings()\n",
    "\t\tself.cherokee_vocabulary.create_mappings()\n",
    "\n",
    "\t\tenglish  = torch.zeros(size=(self.count//2, self.max_length), dtype=int)\n",
    "\t\tcherokee = torch.zeros(size=(self.count//2, self.max_length), dtype=int)\n",
    "\n",
    "\t\tfor i, sen in enumerate(self.english):\n",
    "\t\t\tfor j, v in enumerate(self.english_vocabulary.encode(sen)):\n",
    "\t\t\t\tenglish[i, j] = v\n",
    "\t\t\n",
    "\t\tfor i, sen in enumerate(self.cherokee):\n",
    "\t\t\tfor j, v in enumerate(self.cherokee_vocabulary.encode(sen)):\n",
    "\t\t\t\tcherokee[i, j] = v\n",
    "\n",
    "\t\tself.cherokee, self.english = cherokee, english\n",
    "\n",
    "\n",
    "preprocessor = PreProcessor()\n",
    "preprocessor.get_data()\n",
    "preprocessor.create_tensors()\n",
    "\n",
    "test = word_tokenize('ᎤᎵᎦᎵᏴᎮᎢ ᎠᏴᏤᏂ ᏫᎵᎻ.')\n",
    "\n",
    "assert preprocessor.cherokee_vocabulary.decode(preprocessor.cherokee_vocabulary.encode(test)) == test\n",
    "assert preprocessor.english.shape == preprocessor.cherokee.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cherokee Vocabulary Size: 12790\n",
      "English  Vocabulary Size: 6401\n"
     ]
    }
   ],
   "source": [
    "cherokee_vocab_size, english_vocab_size = len(preprocessor.cherokee_vocabulary.stoi), len(preprocessor.english_vocabulary.stoi)\n",
    "\n",
    "print(f'Cherokee Vocabulary Size: {cherokee_vocab_size}')\n",
    "print(f'English  Vocabulary Size: {english_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75406, 106])\n",
      "torch.Size([75406, 106])\n",
      "torch.Size([75406, 6401])\n"
     ]
    }
   ],
   "source": [
    "cherokee_in, english_in, expected_probabilities = [], [], []\n",
    "\n",
    "for i, c in enumerate(preprocessor.cherokee):\n",
    "\tcherokee_tensor = torch.tensor(list(c))\n",
    "\tfor j in range(1, preprocessor.max_length - 1):\n",
    "\t\tenglish_tensor = torch.zeros(preprocessor.max_length)\n",
    "\t\tenglish_tensor[:j] = preprocessor.english[i, :j]\n",
    "\t\t\n",
    "\t\tprobability = torch.zeros(english_vocab_size)\n",
    "\t\tprobability[preprocessor.english[i, j].item()] = 1\n",
    "\t\tif preprocessor.english[i, j].item() != 0:\n",
    "\t\t\tcherokee_in.append(cherokee_tensor)\n",
    "\t\t\tenglish_in.append(english_tensor)\n",
    "\t\t\texpected_probabilities.append(probability)\n",
    "\n",
    "\n",
    "cherokee, english, expected_probabilities = torch.stack(cherokee_in).int(), torch.stack(english_in).int(), torch.stack(expected_probabilities).float()\n",
    "\n",
    "print(cherokee.shape)\n",
    "print(english.shape)\n",
    "print(expected_probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(cherokee.shape[0])\n",
    "\n",
    "train_cherokee = cherokee[:int(0.8*size)]\n",
    "train_english  = english[:int(0.8*size)]\n",
    "train_probabilities = expected_probabilities[:int(0.8*size)]\n",
    "\n",
    "test_cherokee  = cherokee[int(0.8*size):int(0.9*size)]\n",
    "test_english   = english[int(0.8*size):int(0.9*size)]\n",
    "test_probabilities = expected_probabilities[int(0.8*size):int(0.9*size)]\n",
    "\n",
    "val_cherokee   = cherokee[int(0.9*size):]\n",
    "val_english    = english[int(0.9*size):]\n",
    "val_probabilities = expected_probabilities[int(0.9*size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONS = 64\n",
    "QKV_DIMENSIONS       = 64\n",
    "SEQUENCE_LENGTH      = preprocessor.max_length\n",
    "ATTENTION_HEADS      = 8\n",
    "DECODERS             = 4\n",
    "ENCODERS             = 4\n",
    "BATCH_SIZE           = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1780,  1.4851, -1.3170,  ...,  1.0396, -0.1582, -0.3851],\n",
      "        [ 1.2525, -1.6258, -0.0777,  ..., -0.7738,  0.5700, -1.1544],\n",
      "        [ 0.7286,  0.4564, -1.0594,  ...,  0.4053, -0.8341,  0.5077],\n",
      "        ...,\n",
      "        [ 2.1043,  1.0811, -0.2091,  ...,  1.3817,  0.1229,  0.2036],\n",
      "        [-0.9733,  0.2396, -1.1271,  ...,  0.0310,  0.1468, -2.1297],\n",
      "        [-1.1083,  0.8323,  0.4088,  ..., -0.9980, -0.9937,  0.0900]])\n",
      "tensor([[-1.1780,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "        [ 1.2525, -1.6258,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "        [ 0.7286,  0.4564, -1.0594,  ...,    -inf,    -inf,    -inf],\n",
      "        ...,\n",
      "        [ 2.1043,  1.0811, -0.2091,  ...,  1.3817,    -inf,    -inf],\n",
      "        [-0.9733,  0.2396, -1.1271,  ...,  0.0310,  0.1468,    -inf],\n",
      "        [-1.1083,  0.8323,  0.4088,  ..., -0.9980, -0.9937,  0.0900]])\n"
     ]
    }
   ],
   "source": [
    "def mask_tensor(t):\n",
    "\tmask = torch.tril(torch.ones(size=(t.shape)))\n",
    "\tmask[mask==0], mask[mask==1] = float('-inf'), 0\n",
    "\t\n",
    "\treturn t + mask\n",
    "\n",
    "test = torch.randn(SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n",
    "print(test)\n",
    "print(mask_tensor(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.0545e-02, -5.5631e-02,  1.5919e-01,  ..., -5.1795e-02,\n",
      "           1.9747e-01, -3.6008e-02],\n",
      "         [-7.4673e-02, -5.7112e-02,  1.6675e-01,  ..., -5.1271e-02,\n",
      "           1.7088e-01,  3.2525e-02],\n",
      "         [-1.5145e-01, -3.6001e-02,  2.2089e-01,  ..., -7.2068e-02,\n",
      "           2.3880e-01,  5.7365e-02],\n",
      "         ...,\n",
      "         [-3.2407e-02, -5.0771e-02,  1.4736e-01,  ..., -4.8669e-02,\n",
      "           1.4239e-01,  6.0525e-02],\n",
      "         [-4.7254e-02, -5.2809e-03,  2.3061e-01,  ..., -9.7871e-02,\n",
      "           2.0430e-01,  6.5788e-02],\n",
      "         [-5.3027e-02, -4.7955e-02,  1.2685e-01,  ..., -7.8321e-02,\n",
      "           2.2337e-01, -4.7087e-02]],\n",
      "\n",
      "        [[-7.8454e-02, -2.1723e-02,  1.7472e-01,  ..., -4.8120e-02,\n",
      "           2.0975e-01,  4.8083e-02],\n",
      "         [-4.4241e-02, -1.2681e-02,  1.7373e-01,  ..., -9.8478e-02,\n",
      "           2.0480e-01,  3.3493e-02],\n",
      "         [-6.3327e-02, -2.4110e-02,  1.4606e-01,  ..., -7.6299e-02,\n",
      "           2.2843e-01, -2.1622e-02],\n",
      "         ...,\n",
      "         [-1.2856e-01, -2.9992e-02,  2.2178e-01,  ..., -4.8233e-02,\n",
      "           1.8224e-01,  5.7654e-02],\n",
      "         [-4.1290e-02, -2.5960e-02,  1.7865e-01,  ..., -4.1868e-02,\n",
      "           1.6088e-01,  3.6869e-02],\n",
      "         [-6.7322e-02, -4.6889e-02,  1.5700e-01,  ..., -7.4167e-02,\n",
      "           1.9118e-01,  1.0923e-02]],\n",
      "\n",
      "        [[-1.0616e-01, -6.3040e-02,  1.6930e-01,  ..., -6.8821e-02,\n",
      "           1.9510e-01,  4.6490e-02],\n",
      "         [-2.3601e-02, -1.8093e-02,  2.3446e-01,  ..., -4.0112e-02,\n",
      "           1.8141e-01,  2.9891e-02],\n",
      "         [-6.3882e-02, -2.9099e-02,  1.8165e-01,  ..., -6.3877e-02,\n",
      "           1.5368e-01,  6.1693e-02],\n",
      "         ...,\n",
      "         [-3.4795e-02, -2.0453e-02,  2.1765e-01,  ..., -1.0075e-01,\n",
      "           2.0532e-01,  4.2586e-02],\n",
      "         [-7.7614e-02, -6.3844e-02,  1.2893e-01,  ..., -4.3573e-02,\n",
      "           2.0946e-01, -2.9478e-03],\n",
      "         [-1.8304e-02, -5.4346e-02,  1.4228e-01,  ..., -7.5610e-02,\n",
      "           2.2235e-01,  2.8359e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.5692e-02, -5.7127e-02,  1.7206e-01,  ..., -1.0564e-01,\n",
      "           2.1220e-01,  1.7124e-02],\n",
      "         [-9.3507e-02, -2.3063e-02,  2.1321e-01,  ..., -8.3707e-02,\n",
      "           2.0978e-01,  5.1948e-02],\n",
      "         [-6.8796e-02, -6.3026e-02,  1.9975e-01,  ..., -4.2126e-02,\n",
      "           1.5353e-01,  2.3902e-02],\n",
      "         ...,\n",
      "         [-7.7844e-02,  2.5712e-04,  1.3660e-01,  ..., -3.6288e-02,\n",
      "           2.0032e-01,  1.1995e-02],\n",
      "         [-1.1967e-01, -1.7123e-02,  1.3195e-01,  ..., -4.3544e-02,\n",
      "           1.5591e-01,  1.4239e-02],\n",
      "         [-9.1543e-02, -4.0690e-02,  1.9830e-01,  ..., -4.8303e-02,\n",
      "           1.8877e-01,  1.8708e-02]],\n",
      "\n",
      "        [[-1.3070e-01, -3.8953e-02,  1.8439e-01,  ..., -8.4162e-02,\n",
      "           2.6012e-01,  3.4724e-05],\n",
      "         [-5.4633e-02, -5.9790e-02,  1.5146e-01,  ..., -1.0768e-01,\n",
      "           2.3903e-01,  4.5813e-02],\n",
      "         [-5.0876e-02, -4.3911e-02,  1.8325e-01,  ..., -3.0437e-02,\n",
      "           1.7463e-01, -3.7031e-02],\n",
      "         ...,\n",
      "         [-8.8196e-02, -3.5797e-02,  2.2200e-01,  ..., -8.2355e-02,\n",
      "           2.0772e-01,  4.2730e-02],\n",
      "         [-6.7345e-02, -1.0011e-03,  1.7549e-01,  ..., -1.0182e-01,\n",
      "           2.0427e-01,  4.1170e-02],\n",
      "         [-1.1639e-01, -7.9621e-02,  1.7795e-01,  ..., -6.8538e-02,\n",
      "           2.4860e-01,  4.4445e-03]],\n",
      "\n",
      "        [[-1.2636e-01, -3.5980e-02,  1.5784e-01,  ..., -9.5901e-02,\n",
      "           2.2714e-01,  2.2730e-02],\n",
      "         [-6.4031e-02, -8.1919e-02,  1.8839e-01,  ..., -2.3859e-02,\n",
      "           1.8678e-01,  1.9593e-02],\n",
      "         [-9.9795e-02, -9.0697e-02,  1.2728e-01,  ..., -6.7361e-02,\n",
      "           2.2230e-01, -5.1604e-03],\n",
      "         ...,\n",
      "         [-1.0112e-01, -5.2350e-02,  1.6564e-01,  ..., -5.3765e-02,\n",
      "           2.0085e-01,  1.3312e-02],\n",
      "         [-1.4880e-01, -5.2997e-02,  1.8539e-01,  ..., -4.4656e-02,\n",
      "           1.8370e-01,  6.7702e-02],\n",
      "         [-4.1400e-02, -4.5543e-02,  1.1422e-01,  ..., -8.7121e-02,\n",
      "           1.8303e-01, -2.2779e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\tdef __init__(self, masked=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.obtain_key   = nn.Linear(EMBEDDING_DIMENSIONS, QKV_DIMENSIONS)\n",
    "\t\tself.obtain_query = nn.Linear(EMBEDDING_DIMENSIONS, QKV_DIMENSIONS)\n",
    "\t\tself.obtain_value = nn.Linear(EMBEDDING_DIMENSIONS, QKV_DIMENSIONS)\n",
    "\t\tself.masked = masked\n",
    "\n",
    "\tdef forward(self, data, encoder_output=None):\n",
    "\t\tif encoder_output is None: Q, K, V = self.obtain_query(data), self.obtain_key(data), self.obtain_value(data)\n",
    "\t\telse: Q, K, V = self.obtain_query(data), self.obtain_key(encoder_output), self.obtain_value(encoder_output)\n",
    "\t\tmat_mul = Q @ K.transpose(-2, -1)\n",
    "\t\tscaled_mat_mul = mat_mul / sqrt(QKV_DIMENSIONS)\n",
    "\t\tif self.masked: scaled_mat_mul = mask_tensor(scaled_mat_mul)\n",
    "\t\tsoftmax_mat_mul = torch.softmax(scaled_mat_mul, dim=-1)\n",
    "\t\toutput = softmax_mat_mul @ V\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "test, e_output = torch.randn(size=(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS)), torch.randn(size=(SEQUENCE_LENGTH, QKV_DIMENSIONS))\n",
    "test_module = AttentionHead()\n",
    "print(test_module(test, encoder_output=e_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0097,  0.0286,  0.0925,  ...,  0.1253,  0.0720, -0.0702],\n",
      "         [ 0.0121,  0.0350,  0.1462,  ...,  0.0872,  0.0444, -0.0661],\n",
      "         [-0.0182,  0.0154,  0.1256,  ...,  0.0684,  0.0717, -0.0280],\n",
      "         ...,\n",
      "         [ 0.0106,  0.0477,  0.0930,  ...,  0.0838,  0.0693, -0.0475],\n",
      "         [ 0.0325,  0.0539,  0.1172,  ...,  0.0770,  0.0422, -0.0317],\n",
      "         [ 0.0058,  0.0198,  0.0914,  ...,  0.0942,  0.0651, -0.0626]],\n",
      "\n",
      "        [[ 0.0494,  0.0927,  0.0940,  ...,  0.0379,  0.0245, -0.0365],\n",
      "         [ 0.0501,  0.0349,  0.1080,  ...,  0.0564,  0.0592, -0.0576],\n",
      "         [ 0.0510,  0.0658,  0.0927,  ...,  0.0650,  0.0624, -0.0588],\n",
      "         ...,\n",
      "         [ 0.0476,  0.0587,  0.0786,  ...,  0.0830,  0.0597, -0.0796],\n",
      "         [ 0.0694,  0.0671,  0.1079,  ...,  0.0392,  0.0493, -0.0666],\n",
      "         [ 0.0753,  0.0783,  0.1124,  ...,  0.0462,  0.0639, -0.0583]],\n",
      "\n",
      "        [[-0.0325,  0.0825,  0.0793,  ...,  0.1119,  0.0321,  0.0648],\n",
      "         [-0.0338,  0.0746,  0.0575,  ...,  0.0915,  0.0632,  0.0254],\n",
      "         [-0.0455,  0.0807,  0.0593,  ...,  0.1142,  0.0745,  0.0583],\n",
      "         ...,\n",
      "         [-0.0639,  0.0330,  0.0287,  ...,  0.1202,  0.0871,  0.0531],\n",
      "         [-0.0791,  0.0870,  0.0316,  ...,  0.1045,  0.0647,  0.0765],\n",
      "         [-0.0353,  0.0913,  0.0266,  ...,  0.1059,  0.0555,  0.0431]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0081,  0.1073,  0.0149,  ...,  0.0846,  0.0373,  0.0112],\n",
      "         [-0.0416,  0.0896,  0.0136,  ...,  0.0882,  0.0324, -0.0014],\n",
      "         [-0.0107,  0.0705,  0.0543,  ...,  0.0838,  0.0488, -0.0145],\n",
      "         ...,\n",
      "         [-0.0098,  0.0915,  0.0612,  ...,  0.0904,  0.0382,  0.0275],\n",
      "         [-0.0176,  0.0940,  0.0291,  ...,  0.0823,  0.0434,  0.0338],\n",
      "         [-0.0024,  0.0861,  0.0508,  ...,  0.0881,  0.0578,  0.0078]],\n",
      "\n",
      "        [[ 0.1159,  0.0396,  0.1017,  ...,  0.0593,  0.0975, -0.0837],\n",
      "         [ 0.1132,  0.0323,  0.0786,  ...,  0.0475,  0.0872, -0.0632],\n",
      "         [ 0.1237,  0.0628,  0.0995,  ...,  0.0550,  0.1043, -0.0992],\n",
      "         ...,\n",
      "         [ 0.1017,  0.0223,  0.0655,  ...,  0.0656,  0.1148, -0.1052],\n",
      "         [ 0.0966,  0.0377,  0.0818,  ...,  0.0342,  0.1198, -0.0399],\n",
      "         [ 0.1076,  0.0653,  0.0489,  ...,  0.0519,  0.1181, -0.0754]],\n",
      "\n",
      "        [[ 0.0964,  0.0573,  0.0551,  ...,  0.0796,  0.0966, -0.0061],\n",
      "         [ 0.1020,  0.0484,  0.0635,  ...,  0.0500,  0.0594, -0.0171],\n",
      "         [ 0.0828,  0.0377,  0.0682,  ...,  0.0733,  0.1049, -0.0338],\n",
      "         ...,\n",
      "         [ 0.0794,  0.0612,  0.0505,  ...,  0.0592,  0.0841, -0.0291],\n",
      "         [ 0.1047,  0.0518,  0.0716,  ...,  0.0583,  0.0966, -0.0183],\n",
      "         [ 0.1005,  0.0308,  0.0550,  ...,  0.0829,  0.0608, -0.0557]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\tdef __init__(self, masked=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads  = [AttentionHead(masked=masked) for _ in range(ATTENTION_HEADS)]\n",
    "\t\tself.linear = nn.Linear(EMBEDDING_DIMENSIONS*ATTENTION_HEADS, EMBEDDING_DIMENSIONS)\n",
    "\n",
    "\tdef forward(self, data, encoder_output=None):\n",
    "\t\tvectors = torch.cat([head(data, encoder_output=encoder_output) for head in self.heads], dim=-1)\n",
    "\t\treturn self.linear(vectors)\n",
    "\t\n",
    "\n",
    "test = torch.randn(size=(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS))\n",
    "test_module = MultiHeadedAttention()\n",
    "print(test_module(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.4419e-01,  6.5899e-02, -2.7589e-01,  ..., -1.4690e-01,\n",
      "           2.2770e-01,  9.7680e-02],\n",
      "         [ 1.4047e-01,  8.2262e-02, -4.1861e-01,  ...,  5.4490e-02,\n",
      "          -9.1217e-02,  2.6626e-01],\n",
      "         [ 8.7623e-02,  5.8929e-02, -1.9815e-01,  ..., -1.5461e-01,\n",
      "           1.5912e-01,  2.2302e-01],\n",
      "         ...,\n",
      "         [ 1.0174e-01,  9.6830e-02, -3.6062e-01,  ..., -1.8408e-01,\n",
      "           2.4727e-01,  5.0433e-01],\n",
      "         [ 3.4881e-01, -1.6631e-01, -5.9349e-01,  ..., -2.7988e-01,\n",
      "           2.8831e-01,  3.9413e-01],\n",
      "         [ 2.5956e-01,  3.4238e-01, -4.0262e-01,  ..., -1.0237e-01,\n",
      "          -4.5205e-01,  4.2216e-01]],\n",
      "\n",
      "        [[ 4.1898e-01, -9.2823e-02, -5.3060e-01,  ..., -6.1057e-01,\n",
      "           2.5743e-01,  2.1742e-02],\n",
      "         [-1.5915e-01,  3.3048e-02, -4.0627e-01,  ..., -2.2890e-01,\n",
      "           3.2257e-01,  2.7149e-01],\n",
      "         [-3.9316e-02,  1.4449e-01, -2.6642e-01,  ..., -2.9766e-01,\n",
      "          -3.0109e-02,  9.9205e-02],\n",
      "         ...,\n",
      "         [ 9.3181e-02,  3.1617e-01, -9.5756e-02,  ..., -3.1637e-01,\n",
      "          -1.2817e-02,  3.7674e-01],\n",
      "         [ 1.8627e-01,  2.4648e-01, -2.5872e-01,  ...,  6.4170e-02,\n",
      "           1.0586e-02,  5.3457e-01],\n",
      "         [ 2.2432e-01,  1.8849e-01, -2.5185e-01,  ..., -4.7341e-01,\n",
      "           6.8476e-02,  2.6605e-01]],\n",
      "\n",
      "        [[ 2.7739e-04, -1.3845e-02, -2.6607e-01,  ..., -1.4887e-01,\n",
      "          -1.3297e-01,  6.7093e-02],\n",
      "         [-5.6521e-02,  8.9100e-02,  1.9451e-01,  ..., -2.1061e-01,\n",
      "          -7.2352e-02,  1.3298e-01],\n",
      "         [-4.7747e-02, -2.5699e-01, -2.6535e-01,  ..., -2.4497e-01,\n",
      "           1.5442e-01,  1.7056e-01],\n",
      "         ...,\n",
      "         [-2.3645e-02,  4.3607e-01,  1.5042e-02,  ..., -3.3046e-01,\n",
      "           2.0858e-01,  4.3605e-01],\n",
      "         [ 9.8067e-02,  1.9098e-01, -2.4133e-02,  ..., -2.7404e-01,\n",
      "           2.9702e-01,  1.0008e-01],\n",
      "         [-2.3476e-01,  2.1366e-01, -3.5255e-01,  ..., -2.1946e-01,\n",
      "           1.8292e-01,  3.0313e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9767e-01,  2.0051e-01, -5.4712e-01,  ..., -2.4108e-01,\n",
      "           3.1616e-03,  2.8635e-01],\n",
      "         [-2.6065e-01, -7.9500e-02, -4.0818e-01,  ..., -2.5553e-01,\n",
      "           2.8127e-01,  9.3872e-02],\n",
      "         [ 9.9962e-02,  5.2169e-01, -4.6616e-01,  ..., -3.0749e-01,\n",
      "           2.3929e-01,  4.8682e-01],\n",
      "         ...,\n",
      "         [ 1.4088e-02,  2.2779e-01, -4.1653e-01,  ..., -3.0246e-01,\n",
      "           1.6712e-01,  5.6047e-01],\n",
      "         [ 2.0665e-01, -3.8859e-03,  5.9426e-02,  ..., -3.6182e-01,\n",
      "          -2.3771e-01,  3.5200e-01],\n",
      "         [ 1.5684e-01,  2.1800e-01, -2.3010e-01,  ..., -5.4790e-01,\n",
      "           3.4509e-02,  5.2533e-01]],\n",
      "\n",
      "        [[ 1.4336e-01,  2.2574e-01, -5.9255e-01,  ..., -1.6888e-01,\n",
      "           5.4444e-02,  6.4795e-01],\n",
      "         [-4.8979e-01,  2.4991e-01, -3.9280e-01,  ..., -2.2975e-01,\n",
      "           1.0891e-01,  3.4906e-01],\n",
      "         [ 9.8405e-02,  3.0466e-01, -3.5347e-02,  ..., -2.1653e-02,\n",
      "           2.0212e-02,  3.3768e-01],\n",
      "         ...,\n",
      "         [ 1.6869e-01, -3.3862e-02, -1.1939e-01,  ...,  1.7736e-01,\n",
      "           1.0175e-01,  5.1024e-01],\n",
      "         [-1.6663e-01,  4.5976e-01, -2.6747e-01,  ..., -2.8289e-01,\n",
      "          -1.5800e-01,  3.0825e-01],\n",
      "         [ 2.2415e-01,  5.2807e-01, -5.5585e-01,  ...,  1.9691e-02,\n",
      "          -1.2135e-01,  4.8887e-01]],\n",
      "\n",
      "        [[ 1.4389e-01,  1.7993e-01, -1.6327e-01,  ..., -5.5873e-02,\n",
      "           1.5537e-01,  5.9949e-01],\n",
      "         [-4.8797e-02,  4.8884e-01,  3.0959e-02,  ..., -2.1807e-01,\n",
      "          -9.8346e-02,  1.8474e-01],\n",
      "         [ 9.0272e-02,  1.5841e-01, -2.0755e-01,  ..., -2.6639e-01,\n",
      "          -1.4257e-01,  7.5157e-02],\n",
      "         ...,\n",
      "         [-1.6538e-01, -3.3091e-01, -1.2757e-02,  ..., -3.6989e-01,\n",
      "           1.0708e-01,  2.7364e-01],\n",
      "         [ 4.5247e-02,  1.0425e-01, -5.4853e-01,  ..., -1.0025e-01,\n",
      "          -2.3313e-02,  1.8712e-01],\n",
      "         [ 5.0460e-02, -1.0857e-01, -3.0118e-02,  ..., -2.2840e-01,\n",
      "          -2.2274e-01,  1.1785e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.network = nn.Sequential(\n",
    "\t\t\tnn.Linear(EMBEDDING_DIMENSIONS, EMBEDDING_DIMENSIONS),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(EMBEDDING_DIMENSIONS, EMBEDDING_DIMENSIONS)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\treturn self.network(data)\n",
    "\n",
    "test = torch.randn(size=(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS))\n",
    "test_module = FeedForward()\n",
    "print(test_module(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.8996e-01, -1.6537e+00,  1.2328e+00,  ..., -1.2368e-01,\n",
      "          -1.7143e+00, -6.2563e-01],\n",
      "         [-1.3953e-01, -4.3369e-01, -1.5503e-01,  ...,  1.8496e+00,\n",
      "           3.1170e-01, -9.5862e-01],\n",
      "         [-4.1079e-01,  1.0137e+00,  9.1881e-01,  ...,  6.7879e-01,\n",
      "          -6.1745e-01,  1.0037e+00],\n",
      "         ...,\n",
      "         [-1.6973e+00, -9.8921e-01, -2.3452e-01,  ...,  9.9356e-01,\n",
      "          -5.4330e-01,  5.8571e-01],\n",
      "         [ 1.0433e+00,  2.9293e-01, -1.6109e+00,  ...,  6.6787e-01,\n",
      "           3.7960e-02,  1.5968e+00],\n",
      "         [-1.0673e+00,  4.8107e-01,  1.5464e+00,  ...,  2.3817e-01,\n",
      "          -2.1027e+00,  1.1608e+00]],\n",
      "\n",
      "        [[-6.4214e-01, -1.3467e-01,  4.8817e-01,  ...,  9.3230e-01,\n",
      "          -1.4842e+00,  1.6568e+00],\n",
      "         [-1.6590e+00, -6.7137e-01, -1.3637e-01,  ..., -1.9541e-03,\n",
      "          -6.0459e-01, -3.3372e+00],\n",
      "         [ 9.7985e-01, -1.1091e+00,  1.6497e+00,  ..., -1.0765e-01,\n",
      "          -1.8404e+00,  2.4165e-01],\n",
      "         ...,\n",
      "         [-4.2877e-01, -1.1019e+00, -1.0240e+00,  ..., -5.3879e-01,\n",
      "           1.5177e+00,  1.0745e+00],\n",
      "         [-9.4852e-01,  1.0331e+00,  8.2848e-01,  ...,  2.1864e+00,\n",
      "          -7.3693e-01,  5.0219e-01],\n",
      "         [ 3.3355e-01, -2.1256e+00, -1.3111e+00,  ...,  6.8426e-01,\n",
      "           1.1273e+00, -4.0056e-01]],\n",
      "\n",
      "        [[-4.7972e-01,  8.9035e-02,  7.4009e-01,  ...,  3.0243e-01,\n",
      "          -6.5829e-01, -2.0998e+00],\n",
      "         [-5.9834e-01,  1.0374e+00, -7.9276e-01,  ..., -6.4262e-01,\n",
      "           1.6845e+00, -5.1692e-01],\n",
      "         [ 2.9963e-02,  4.8408e-01,  1.1661e+00,  ...,  2.3624e+00,\n",
      "           5.1092e-01, -4.3753e-01],\n",
      "         ...,\n",
      "         [-1.2667e+00, -1.0115e+00, -4.2019e-01,  ...,  4.5692e-01,\n",
      "          -5.8913e-02,  1.8322e+00],\n",
      "         [ 1.1565e+00,  1.9881e-01, -1.8713e-01,  ...,  1.1817e+00,\n",
      "           2.5336e-01,  1.7068e+00],\n",
      "         [ 7.5321e-01, -1.3916e+00,  7.0081e-01,  ...,  2.3311e-01,\n",
      "          -1.0597e+00, -5.3845e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0215e-01,  2.5210e-01, -7.1732e-01,  ...,  9.1621e-01,\n",
      "           1.6658e+00,  1.1799e+00],\n",
      "         [-1.0924e+00,  1.1254e+00,  6.5891e-01,  ..., -2.2401e-01,\n",
      "          -9.1796e-01, -6.7680e-01],\n",
      "         [ 6.7191e-01,  9.0516e-01,  1.0130e+00,  ...,  1.9478e+00,\n",
      "           7.8374e-01,  1.5249e-02],\n",
      "         ...,\n",
      "         [ 2.4130e+00, -1.2548e+00, -4.4566e-01,  ...,  1.1322e+00,\n",
      "           6.9955e-01, -6.3457e-01],\n",
      "         [-4.7686e-01, -1.8683e-01, -2.7139e-01,  ..., -4.7643e-01,\n",
      "           3.5277e-01, -3.0486e+00],\n",
      "         [-6.3546e-01,  2.6288e-02,  2.4222e-01,  ...,  1.2583e+00,\n",
      "          -9.6472e-01,  2.4922e+00]],\n",
      "\n",
      "        [[-1.2384e+00, -8.1984e-01, -1.0139e+00,  ..., -1.4033e-01,\n",
      "           1.1402e-01, -4.9245e-01],\n",
      "         [ 1.9002e-01,  1.1730e+00,  1.4499e+00,  ...,  3.1911e-01,\n",
      "           6.6006e-01,  2.6752e+00],\n",
      "         [-7.2577e-01,  1.3375e+00, -7.3793e-01,  ..., -1.4293e+00,\n",
      "           7.1163e-01,  1.1694e+00],\n",
      "         ...,\n",
      "         [-1.4084e-01, -1.0965e+00,  1.5739e-01,  ..., -3.3935e-01,\n",
      "          -1.0699e+00, -2.7223e-01],\n",
      "         [ 6.4526e-01, -5.9153e-01,  2.5315e-01,  ..., -1.4547e+00,\n",
      "          -1.0263e+00, -4.4878e-01],\n",
      "         [-6.6638e-01, -1.9080e-01, -2.9882e-01,  ..., -2.6239e-01,\n",
      "           6.4087e-01, -1.2222e-01]],\n",
      "\n",
      "        [[-5.4306e-01, -9.9757e-01,  1.1739e+00,  ...,  9.9591e-01,\n",
      "          -1.2552e+00, -1.2270e-01],\n",
      "         [ 8.2293e-01,  6.7763e-01,  5.6318e-01,  ...,  1.1881e+00,\n",
      "           5.6974e-01, -6.0899e-01],\n",
      "         [-8.3432e-01,  2.6954e-01, -1.5145e-01,  ...,  1.3234e-01,\n",
      "           2.0103e-01,  1.4849e+00],\n",
      "         ...,\n",
      "         [ 8.7225e-01, -3.9331e-01, -1.2636e+00,  ..., -1.2156e+00,\n",
      "          -1.0549e+00, -1.3750e+00],\n",
      "         [ 7.9552e-01, -2.0498e+00,  2.3168e-01,  ..., -6.5752e-01,\n",
      "          -6.9412e-01, -1.7719e-01],\n",
      "         [-9.6107e-01, -1.3993e+00, -2.8153e-01,  ..., -1.0282e+00,\n",
      "           3.2205e-01, -7.1266e-01]]])\n",
      "tensor([[[ 0.6929, -2.2727, -1.0315,  ...,  0.2427, -1.1241, -0.7460],\n",
      "         [ 0.1780,  0.1641, -3.1379,  ...,  2.7160, -0.2400, -2.4109],\n",
      "         [-0.9099,  1.9923, -2.4132,  ...,  1.3939, -1.0973, -0.0387],\n",
      "         ...,\n",
      "         [-0.3416, -0.7142, -4.5671,  ...,  1.7472, -0.3145, -1.1650],\n",
      "         [ 2.6410,  0.3763, -4.4363,  ...,  0.4734,  0.4604,  2.0903],\n",
      "         [-0.5978,  0.4007, -0.6987,  ...,  0.4812, -1.5042,  0.5927]],\n",
      "\n",
      "        [[ 1.1106,  1.7258, -1.7405,  ...,  0.2462, -3.3995,  0.7424],\n",
      "         [-0.0542,  0.3465, -1.2819,  ...,  0.1337, -1.3553, -2.8120],\n",
      "         [ 3.5684,  0.2044,  0.3611,  ..., -0.9635, -3.1001, -2.0951],\n",
      "         ...,\n",
      "         [ 2.0322, -0.2045, -2.8013,  ..., -1.3001, -0.6224,  0.4876],\n",
      "         [-1.2115,  1.9091, -0.7899,  ...,  2.3763, -2.2889, -0.6026],\n",
      "         [ 1.6634, -1.3074, -2.1043,  ...,  1.1790,  0.6621, -0.5846]],\n",
      "\n",
      "        [[ 0.9886,  1.2700, -2.9346,  ..., -1.1508, -3.2092, -3.1507],\n",
      "         [-0.2180,  1.4379, -3.4665,  ..., -0.1514, -0.3887,  0.1446],\n",
      "         [ 0.7120,  0.7821, -1.2229,  ...,  2.0259, -0.2854, -1.4297],\n",
      "         ...,\n",
      "         [-0.2947,  0.5763, -4.1940,  ..., -0.7905, -1.3149,  0.8802],\n",
      "         [ 2.5968,  0.8669, -1.6005,  ...,  0.6707, -2.2318,  1.5519],\n",
      "         [ 2.3173, -0.0489, -1.7405,  ..., -0.4788, -2.8282, -2.6765]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3466,  2.8134, -4.1281,  ..., -0.8170,  0.7160,  1.4138],\n",
      "         [-0.1439,  3.1160, -3.7124,  ..., -0.3640, -1.6589,  0.5858],\n",
      "         [ 1.9660,  2.0517, -2.8914,  ...,  1.6821, -0.6213,  0.7620],\n",
      "         ...,\n",
      "         [ 3.8730,  0.1394, -4.4056,  ...,  1.9395, -1.1443, -1.6477],\n",
      "         [ 1.1041,  1.2569, -5.1531,  ..., -0.6729, -1.9674, -3.8343],\n",
      "         [ 0.2190,  1.6214, -4.5045,  ..., -0.0720, -1.9873,  2.9509]],\n",
      "\n",
      "        [[ 0.6339, -0.2601, -3.6547,  ..., -1.1210, -0.3107, -0.3164],\n",
      "         [ 2.4670,  1.5512, -1.9676,  ..., -0.4620, -0.9351,  2.1587],\n",
      "         [ 1.1706,  2.5588, -2.7951,  ..., -2.8299, -0.1155,  0.7860],\n",
      "         ...,\n",
      "         [ 1.5421, -0.9753, -3.0831,  ..., -0.9053, -2.1222, -0.3450],\n",
      "         [ 2.4351, -1.1383, -2.0147,  ..., -2.1911, -1.8256, -1.0998],\n",
      "         [ 1.4342, -0.0795, -2.5176,  ..., -0.1946, -0.8588, -0.3798]],\n",
      "\n",
      "        [[ 1.1942, -2.4310, -3.5830,  ...,  0.3684, -3.2025,  0.1322],\n",
      "         [ 2.2328,  0.6727, -3.8079,  ...,  1.6761, -1.2682,  0.7720],\n",
      "         [ 1.4825, -0.2810, -4.4145,  ..., -0.1429, -1.1105,  2.0726],\n",
      "         ...,\n",
      "         [ 1.5944, -0.9114, -5.2109,  ..., -2.5814, -3.0408, -1.3150],\n",
      "         [ 3.5919, -2.5510, -1.8679,  ..., -2.1352, -1.3918, -0.9129],\n",
      "         [ 0.0371, -0.8219, -4.8229,  ..., -1.8565, -1.3710,  0.6770]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.multi_headed_attention = MultiHeadedAttention()\n",
    "\t\tself.norm1 = nn.LayerNorm(EMBEDDING_DIMENSIONS)\n",
    "\t\tself.feed_forward = FeedForward()\n",
    "\t\tself.norm2 = nn.LayerNorm(EMBEDDING_DIMENSIONS)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\tattention_vectors = self.multi_headed_attention(data)\n",
    "\t\tnormalised = self.norm1(attention_vectors) + data # Residual Connection\n",
    "\t\tfed_through = self.feed_forward(normalised)\n",
    "\t\tnormalised_2 = self.norm2(fed_through) + normalised\n",
    "\n",
    "\t\treturn normalised_2\n",
    "\t\n",
    "test = torch.randn(size=(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS))\n",
    "print(test)\n",
    "test_module = Encoder()\n",
    "print(test_module(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.7892e+00,  5.6026e-01, -8.0965e-01,  ..., -3.1768e+00,\n",
      "           3.5851e+00, -4.1023e+00],\n",
      "         [-2.0228e+00,  2.2488e+00,  1.8469e-01,  ..., -3.1232e+00,\n",
      "           2.8538e-02, -1.4859e+00],\n",
      "         [-1.1687e+00,  1.4930e+00, -1.3627e+00,  ...,  3.2546e-01,\n",
      "          -7.8101e-01, -3.2138e+00],\n",
      "         ...,\n",
      "         [-2.6029e+00, -3.0364e-01,  1.2680e+00,  ...,  9.5971e-02,\n",
      "          -2.2976e+00, -2.7613e+00],\n",
      "         [-7.7358e-01,  3.2063e+00,  6.5864e-01,  ..., -7.8532e-02,\n",
      "          -2.3526e+00, -9.0300e-01],\n",
      "         [-4.8518e-01,  1.1057e+00,  1.4585e+00,  ..., -3.6011e-01,\n",
      "          -7.5788e-01, -2.3041e+00]],\n",
      "\n",
      "        [[-1.2609e+00,  1.0717e+00,  6.4770e-01,  ...,  1.4167e+00,\n",
      "           1.3495e+00, -1.0517e+00],\n",
      "         [-1.3596e+00,  2.9197e+00,  3.0889e+00,  ...,  1.3776e+00,\n",
      "           1.2732e+00, -1.6888e+00],\n",
      "         [ 8.8238e-02,  2.1040e+00,  2.0326e+00,  ..., -5.5462e-01,\n",
      "          -4.6152e-01, -2.1493e+00],\n",
      "         ...,\n",
      "         [-2.2370e+00,  4.6182e-01,  3.4777e+00,  ..., -1.5528e-01,\n",
      "           7.1923e-01, -3.5278e+00],\n",
      "         [-7.1140e-01, -3.5231e-01,  1.9634e+00,  ..., -8.9620e-01,\n",
      "           9.6171e-01, -8.3461e-01],\n",
      "         [-3.5474e+00,  7.6903e-01,  1.5475e+00,  ..., -1.3794e+00,\n",
      "          -3.9610e-01, -3.6049e+00]],\n",
      "\n",
      "        [[-2.3945e+00,  1.7745e+00,  2.1480e+00,  ...,  3.0194e+00,\n",
      "          -1.0039e+00, -2.1547e+00],\n",
      "         [-5.3807e-01,  4.2793e+00,  1.7143e+00,  ...,  2.8949e+00,\n",
      "           2.5192e+00, -2.2111e+00],\n",
      "         [-9.9782e-01,  1.3514e+00, -1.1503e+00,  ...,  1.9840e+00,\n",
      "           1.9513e-01, -1.0867e+00],\n",
      "         ...,\n",
      "         [-4.0689e-01,  5.7129e+00,  1.2331e+00,  ...,  1.1193e+00,\n",
      "           1.2681e+00,  7.5460e-02],\n",
      "         [-1.5071e+00,  1.6591e+00,  2.3612e+00,  ..., -2.9937e-01,\n",
      "           2.4840e+00, -2.0776e+00],\n",
      "         [-5.1769e+00,  3.8123e+00,  6.8306e-01,  ...,  2.3706e+00,\n",
      "           5.9883e-01, -2.4698e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2683e+00, -1.2862e+00,  1.0930e+00,  ...,  4.5819e+00,\n",
      "           1.8877e+00, -1.0739e+00],\n",
      "         [-2.2030e+00, -3.9393e-01, -9.8520e-01,  ...,  3.7673e+00,\n",
      "           8.6522e-01, -3.0285e+00],\n",
      "         [-2.6202e+00, -3.6488e-01,  1.3884e+00,  ...,  1.5541e+00,\n",
      "          -2.5037e-01, -3.9389e+00],\n",
      "         ...,\n",
      "         [-3.6967e+00,  2.6632e+00,  2.9717e+00,  ...,  2.2965e+00,\n",
      "           1.0681e+00, -2.3674e+00],\n",
      "         [-2.3551e+00,  2.0724e+00,  4.9823e+00,  ...,  3.1633e+00,\n",
      "           1.2112e+00, -4.4342e+00],\n",
      "         [-3.3758e+00, -3.8072e-01,  2.1774e+00,  ...,  3.1281e+00,\n",
      "          -6.0389e-01, -2.3414e+00]],\n",
      "\n",
      "        [[-2.6460e+00, -5.7009e-01, -1.4262e+00,  ...,  1.7572e-01,\n",
      "           1.2589e-01, -2.3706e+00],\n",
      "         [-6.9084e-01,  1.5458e+00,  1.3686e+00,  ...,  1.3089e+00,\n",
      "           2.8043e+00, -2.3303e+00],\n",
      "         [-2.1702e+00, -3.6488e-01,  1.1091e-01,  ...,  1.3027e+00,\n",
      "           1.3646e+00, -9.8152e-01],\n",
      "         ...,\n",
      "         [-2.3899e+00, -1.2087e-02, -2.8658e-01,  ...,  1.9470e+00,\n",
      "          -8.8895e-01, -1.0658e+00],\n",
      "         [-2.5915e+00,  1.6759e+00,  9.5963e-01,  ...,  1.7795e+00,\n",
      "          -1.1194e+00, -2.6332e+00],\n",
      "         [-1.4264e+00,  4.5000e+00,  1.2481e+00,  ...,  1.6344e-01,\n",
      "           5.0502e-01, -6.2267e-01]],\n",
      "\n",
      "        [[-1.6868e+00,  3.7087e+00, -9.2158e-01,  ..., -3.4395e-01,\n",
      "           3.3131e+00, -3.3990e-01],\n",
      "         [-3.5671e+00,  2.4398e+00,  6.6577e-01,  ..., -1.7669e+00,\n",
      "           6.9013e-01, -2.5774e+00],\n",
      "         [-6.2115e-01, -7.1121e-01, -1.5535e-01,  ..., -2.4508e+00,\n",
      "           2.9533e+00,  2.1474e-01],\n",
      "         ...,\n",
      "         [-1.3191e+00, -2.2664e+00,  2.9873e+00,  ..., -2.1169e+00,\n",
      "           3.9977e-03, -2.3021e+00],\n",
      "         [-1.0957e+00,  1.8159e+00,  4.7983e+00,  ..., -3.9639e-01,\n",
      "           4.8696e-01, -1.3456e+00],\n",
      "         [-5.7332e-01,  1.2815e+00,  3.8437e+00,  ...,  5.7467e-01,\n",
      "          -2.6487e-01, -1.1057e+00]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.masked_attention = MultiHeadedAttention(masked=True)\n",
    "\t\tself.norm1 = nn.LayerNorm(EMBEDDING_DIMENSIONS)\n",
    "\t\tself.cross_attention  = MultiHeadedAttention()\n",
    "\t\tself.norm2 = nn.LayerNorm(EMBEDDING_DIMENSIONS)\n",
    "\t\tself.feed_forward = FeedForward()\n",
    "\t\tself.norm3 = nn.LayerNorm(EMBEDDING_DIMENSIONS)\n",
    "\n",
    "\tdef forward(self, data, encoder_output):\n",
    "\t\tattention_vectors = self.masked_attention(data)\n",
    "\t\tnormalised = self.norm1(attention_vectors) + data\n",
    "\t\tcross_attention = self.cross_attention(normalised, encoder_output=encoder_output)\n",
    "\t\tnormalised = self.norm2(cross_attention) + normalised\n",
    "\t\tlinear = self.feed_forward(normalised)\n",
    "\t\tnormalised = self.norm3(linear) + normalised\n",
    "\n",
    "\t\treturn normalised\n",
    "\n",
    "\n",
    "test = torch.randn(size=(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS))\n",
    "encoder_output = torch.randn(size=(BATCH_SIZE, SEQUENCE_LENGTH, QKV_DIMENSIONS))\n",
    "test_module = Decoder()\n",
    "print(test_module(test, encoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 106]) torch.Size([256, 106])\n",
      "torch.Size([256, 6401])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cherokee_embeddings = nn.Embedding(cherokee_vocab_size, EMBEDDING_DIMENSIONS)\n",
    "\t\tself.cherokee_positional_encodings = nn.Embedding(SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS)\n",
    "\t\tself.encoders = [Encoder() for _ in range(ENCODERS)]\n",
    "\t\tself.decoders = [Decoder() for _ in range(DECODERS)]\n",
    "\t\tself.english_embeddings = nn.Embedding(english_vocab_size, EMBEDDING_DIMENSIONS)\n",
    "\t\tself.english_positional_encodings = nn.Embedding(SEQUENCE_LENGTH, EMBEDDING_DIMENSIONS)\n",
    "\t\tself.linear   = nn.Linear(SEQUENCE_LENGTH, english_vocab_size) \n",
    "\n",
    "\tdef forward(self, source, target_context):\n",
    "\t\tencoder_output = self.cherokee_embeddings(source) + self.cherokee_positional_encodings(torch.tensor([i for i in range(SEQUENCE_LENGTH)]))\n",
    "\n",
    "\t\tfor encoder in self.encoders:\n",
    "\t\t\tencoder_output = encoder(encoder_output)\n",
    "\n",
    "\t\tdecoder_output = self.english_embeddings(target_context) + self.english_positional_encodings(torch.tensor([i for i in range(SEQUENCE_LENGTH)]))\n",
    "\n",
    "\t\tfor decoder in self.decoders:\n",
    "\t\t\tdecoder_output = decoder(decoder_output, encoder_output=encoder_output)\n",
    "\n",
    "\t\tdecoder_output = torch.mean(decoder_output, dim=-1)\n",
    "\t\tlogits = self.linear(decoder_output)\n",
    "\t\tprobabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "\t\treturn probabilities\n",
    "\t\n",
    "cherokee_input = torch.randint(low=0, high=cherokee_vocab_size-1, size=(BATCH_SIZE, SEQUENCE_LENGTH,))\n",
    "target_context = torch.zeros(size=(BATCH_SIZE, SEQUENCE_LENGTH,), dtype=int)\n",
    "print(cherokee_input.shape, target_context.shape)\n",
    "model = Transformer()\n",
    "print(model(cherokee_input, target_context).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interface:\n",
    "\tdef __init__(self, learning_rate, model):\n",
    "\t\tself.model = model\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\t\tself.losses = []\n",
    "\n",
    "\tdef get_batch(self, english, cherokee, probabilities):\n",
    "\t\tindexes = torch.randint(0, english.shape[0], size=(BATCH_SIZE,))\n",
    "\t\tenglish  = torch.stack([english[i] for i in indexes])\n",
    "\t\tcherokee = torch.stack([cherokee[i] for i in indexes])\n",
    "\t\tprobabilities = torch.stack([probabilities[i] for i in indexes])\n",
    "\n",
    "\t\treturn english, cherokee, probabilities\n",
    "\t\n",
    "\tdef pass_batch(self, english, cherokee, probabilities):\n",
    "\t\tenglish, cherokee, probabilities = self.get_batch(english, cherokee, probabilities)\n",
    "\t\tlogits = self.model(cherokee, english)\n",
    "\t\tloss = nn.functional.cross_entropy(logits, probabilities.float())\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\tdef plot_loss(self):\n",
    "\t\tfig = px.line(x=range(len(self.losses)), y=self.losses, title='Line Graph of Losses')\n",
    "\t\tfig.update_xaxes(title_text='Data Point Index')\n",
    "\t\tfig.update_yaxes(title_text='Loss Values')\n",
    "\t\tfig.show()\n",
    "\t\n",
    "\tdef train(self, epochs):\n",
    "\t\tfor i in range(epochs):\n",
    "\t\t\tlogits, loss = self.pass_batch(train_english, train_cherokee, train_probabilities)\n",
    "\t\t\tself.optimizer.zero_grad(set_to_none=True)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\tself.losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7642, grad_fn=<DivBackward1>)\n",
      "tensor(8.7639, grad_fn=<DivBackward1>)\n",
      "tensor(8.7620, grad_fn=<DivBackward1>)\n",
      "tensor(8.7551, grad_fn=<DivBackward1>)\n",
      "tensor(8.7407, grad_fn=<DivBackward1>)\n",
      "tensor(8.7152, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "trainer = Interface(learning_rate=0.05, model=model)\n",
    "trainer.train(100)\n",
    "trainer.plot_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
